{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving/","title":"Serving Tools Comparison","text":""},{"location":"serving/#serving-tools-comparison","title":"Serving Tools Comparison","text":"<p>A serving tool is one that allows you to wrap a model into a service that will  be run a process locally or remotely in a cloud setting. Some serving tools like  mlserver, cog, bentoml, and truss give you functionalities to package your serving  code into a container with all of its dependencies so that it can be used in any platform.</p> <p>Another contender in the serving space is Ray and its <code>serve</code> module. Serve works a bit differently  then the packages mentioned above and, in particular, it differs from mlserver in that it is  a framework rather than a plain python package. Let's unpack that a bit.</p> <p>TL;DR MLServer can be used alongside ray while providing additional functionalities not available in  ray's serve module. Ray is more friendly to data practitioners in that it assumes users have a model and  an endpoint to send jobs to and get results from. MLServer is more agnostic and makes less  assumptions about the user's skillset. Ray serve has better documentation and more use cases  have been tried with.</p>"},{"location":"serving/#differences","title":"Differences","text":"<ul> <li>Ray is a framework and mlserver is a Python package. This means that the former has specific  ways of achieving an outcome within a set of constrains so that, for example, your model serving  logic integrates nicely with the other tools available in the framework's ecosystem. The latter has a recipe  for doing one thing well, packaging and serving models, in a specific yet up-to-the-user way.</li> <li>When using Ray, you have a cluster up and running and you can send an application with your model artifact  logic inside a Python file and a cli command. With mlserver, you can package the model with all of its  dependencies in a docker container and send it to any kind of cluster, a serverless function, or elsewhere. This  is a subtle and yet important difference. A container or dockerfile can be sent everywhere an anywhere  by developers or data practitioners that might not know ray or have a cluster available.</li> <li>MLServer uses a standard API definition, the V2 Inference Protocol, while ray serve has no  particular API standard. This neither a good or a bad thing. Having standards helps keep things  organized. Think of this as driving on the left versus the right, with no standards, everyone would  drive on whichever side they feel most comfortable with, but with standards, everyone would feel more  safe knowing that they can expect drivers to be on one side only.</li> <li>Once connected to a cluster, ray allows you to do fractional scaling, which means you can take 30% of a CPU  or GPU for a model and use the rest for some other piece of your application. MLServer does not orchestrate the  resources of your clusters but its main parent, Seldon Core, does allow you to do this in a straightforward way.</li> <li>Ray server allows you to build an inference graph by making the output of one model be the input of another.  You can do this with mlserver and also with Seldon Core or KServe once either have access to your mlserver container.</li> <li>It is not clear from the documentation that ray serve supports grpc while mlserver supports this out of the box.</li> <li>MLServer lives individually in an instance, hence it needs to be manually scaled horizontally/vertically or via a configuration  file in your deployment platform of choice. Ray serve let's you define how many instances you want per individual  model file. This means that ray needs the cluster to be set up in order to send these copies to and from the same  place, say, your laptop, while mlserver relies on other tools to be scaled vertically or horizontally. But, this also  means ray can in theory send copies of mlserver to different instances.</li> <li>Once connected to a cluster, ray can find and orchestrate the resources needed or required to serve a model. If mlserver  doesn't have enough resources to meet the requirements it needs to serve a model, the file will fail.</li> <li>MLServer is natively integrated fastapi and pydantic while ray serve gives you different serving options such as starlette,  fastapi, gradio, and your custom server.</li> <li>You can integrate Gradio UI's within you ray serve application but you cannot create multiple replicas of it (according to  the documentation). It is not clear whether this is possible with mlserver (more on this soon).</li> <li>With ray serve, you can hosts different models within the same instance and send the same request to both models with one  API call. This can be quite useful for testing the output of two different models.</li> </ul>"},{"location":"serving/#similarities","title":"Similarities","text":"<ul> <li>Both tools allow you to create different replicas of a model in the same process.</li> <li>Both provide adaptive batching and online inference support.</li> <li>You can create inference graphs with either</li> <li>Both tools have integrations with different key libraries such as XGBoost, although mlserver has more integrations available.</li> </ul>"},{"location":"serving/00_serving_tools/","title":"MLServer vs Ray Serve","text":""},{"location":"serving/00_serving_tools/#overview-of-mlserver-and-ray-serve","title":"Overview of MLServer and Ray Serve","text":"<p>The following comparison between MLServer and Ray Serve covers different use cases  and scenarios to highlight the strengths and drawbacks of each. You can follow along  in a notebook locally, on Google Colab, or via a GitHub Codespace using the boxes  above.</p> <p>Let's get started!</p>"},{"location":"serving/00_serving_tools/#installation","title":"Installation","text":"MLServerRay Serve <pre><code>pip install mlserver mlserver-huggingface\n</code></pre> <pre><code>pip install \"ray[serve]\" transformers\n</code></pre>"},{"location":"serving/00_serving_tools/#serving-a-model-with-ray","title":"Serving a Model with Ray","text":"<p>With Ray you can create an app as a regular Python file and define a class that  will load your model and run inference with it. The class will get a <code>@serve</code> decorator  to indicate to ray what object in your Python file will become a service. Here is the  example found in the docs here.</p> <pre><code># File name: serve_deployment.py\nfrom starlette.requests import Request\nfrom transformers import pipeline\nfrom ray import serve\nimport ray\n\n\n@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 0.2, \"num_gpus\": 0})\nclass Translator:\n    def __init__(self):\n        # Load model\n        self.model = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n\n    def translate(self, text: str) -&gt; str:\n        # Run inference\n        model_output = self.model(text)\n\n        # Post-process output to return only the translation text\n        translation = model_output[0][\"translation_text\"]\n\n        return translation\n\n    async def __call__(self, http_request: Request) -&gt; str:\n        english_text: str = await http_request.json()\n        return self.translate(english_text)\n\n\ntranslator_app = Translator.bind()\n</code></pre> <p>Then you can serve the app with the following command. The thing to keep in mind is that,  unless the file containing your serving logic has been installed as a package in your development  environment, you will need to move into the directory containing the script so that ray can import  the class.</p> <pre><code>serve run serve_quickstart:translator_app\n</code></pre>"},{"location":"serving/00_serving_tools/#serving-a-model-with-mlserver","title":"Serving a Model with MLServer","text":"<p>Following the <code>transformers</code> example of ray, MLServer has integrations with different frameworks  and the hugging face <code>transformers</code> library is certainly one of them. To create a hugging face model  to be served with MLServer, you can write a <code>model-settings.json</code> file and specify the pipeline you  are trying to create.</p> <p><pre><code># model-settings.json\n{\n\"name\": \"transformer\",\n\"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n\"parameters\": {\n\"extra\": {\n\"task\": \"text-generation\",\n\"pretrained_model\": \"distilgpt2\"\n}\n}\n}\n</code></pre> If you want to have two replicas to match the ray example, you can add this to a <code>settings.json</code> as following.</p> <pre><code># settings.json\n{\nparallel_workers: 2\n}\n</code></pre> <p>Then you can run your model from your CLI using the following command.</p> <pre><code>mlserver start .\n</code></pre> <p>There are also custom ways to build hugging face models.</p> <pre><code># my_summarizer.py\nfrom mlserver import MLServer, Settings, ModelSettings, MLModel\nfrom mlserver.types import InferenceResponse, ResponseOutput\nfrom transformers import pipeline\n\nclass Summarizer(MLModel):\n    async def load(self):\n        self.model = pipeline(\"summarization\", model=\"t5-small\")\n\n    async def predict(self, payload):\n        text = payload.inputs[0].data[0]\n        model_output = self.model(text, min_length=5, max_length=15)\n        response_output = ResponseOutput(\n            name='new_text',\n            shape=[1],\n            datatype='BYTES',\n            data=model_output[0]['summary_text'],\n        )\n\n        return InferenceResponse(model_name='test_model', outputs=[response_output])\n</code></pre> <p>You will need to have the <code>model-settings.json</code> file available in the same directory.</p> <pre><code># model-settings.json\n{\n\"name\": \"my_model\",\n\"implementation\": \"my_summarizer.HuggingFaceRuntime\"\n}\n</code></pre> <p>Then you can run the server with your model using the mlserver cli.</p> <pre><code>mlserver start .\n</code></pre> <p>Lastly, you can also run mlserver as a Python file</p> <pre><code># my_summarizer.py\nfrom mlserver import MLServer, Settings, ModelSettings, MLModel\nfrom mlserver.types import InferenceResponse, ResponseOutput\nfrom transformers import pipeline\nimport asyncio\n\nclass Summarizer(MLModel):\n    async def load(self):\n        self.model = pipeline(\"summarization\", model=\"t5-small\")\n\n    async def predict(self, payload):\n        text = payload.inputs[0].data[0]\n        model_output = self.model(text, min_length=5, max_length=15)\n        response_output = ResponseOutput(\n            name='new_text',\n            shape=[1],\n            datatype='BYTES',\n            data=model_output[0]['summary_text'],\n        )\n\n        return InferenceResponse(model_name='test_model', outputs=[response_output])\n\nasync def main():\n    settings = Settings(debug=True)\n    my_server = MLServer(settings=settings)\n    implementation = ModelSettings(name='awesome_model', implementation=Summarizer)\n    await my_server.start(models_settings=[implementation])\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n</code></pre> <p>You can run it with</p> <pre><code>python my_summarizer.py\n</code></pre>"}]}